---
layout: "base.njk"
title: "LLM Fine Tuning 그리고 RAG 이해하기"
date: 2025-06-11
wip: true
---


이제 LLM(Large Language Model)은 우리에게 굉장히 익숙한 용어가 되었습니다. 이와 더불어 Fine Tuning이라는 용어도 나오고 Rag라는 용어도 LLM과 같이 볼 수 있는데요. 이 글에서 각각 어떤 것인지 차근차근 알아보고 또 각 기술들의 오해를 하고 있는 것은 무엇인지 알아보겠습니다.

## LLM
이제는 우리에게 굉장히 익숙한 LLM은 Large Language Model의 약자입니다. 수십조 단위의 텍스트 데이터를 학습하여 문장을 생성하는 모델입니다. 인간은 할 수 없는 범위를 넘은 학습을 통해서 이제는 인간보다 더 빠르고 정교한 문장을 생성할 수 있게 되었습니다.

LLM이 인간을 뛰어넘을 정도로 똑똑한 문장들을 생성하지만 결국 LLM도 사실 진정한 AI라고는 할 수 없습니다. LLM은 사실 모델 내부에 저장된 데이터를 기반으로 문장을 생성하는 모델일 뿐이고, 자신이 학습을 하지 않은 영역에 대해서는 똑똑한 답변을 해줄 수 없다는 한계가 있습니다.

### LLM에 대한 오해

#### 1. "사람들이 Chat GPT를 많이 쓰면 Chat GPT의 모델들은 점점 똑똑해진다?"

예를 들어, 사람들이 Chat GPT를 많이 쓰면 Chat GPT의 모델들은 점점 똑똑해진다고 생각하는 것이 있습니다. 하지만 사실 이는 전혀 사실이 아닙니다. 기본적으로 GPT 모델이나 Claude 모델 대부분은 사용자와의 대화나 입력으로 실시간으로 학습하지 않습니다.

- 사용자의 대화는 모델 학습에 바로 반영되지 않음
- 대화 세션 안에서는 문맥을 기억하지만, 세션이 종료되면 초기화됩니다.
- 학습을 하려면 별도의 파인 튜닝이나 데이터 재학습 과정이 필요함

따라서 다음과 같은 질문은 오해에 가깝습니다:

> "같은 질문을 반복해서 하면 더 나은 답을 하게 되나요?"
> → 아닙니다. 같은 질문이면 같은 조건에서는 같은 답을 할 확률이 높습니다.

> "회사 데이터를 많이 입력하면 알아서 학습하겠죠?"
> → 그렇지 않습니다. 학습을 하려면 별도로 정제된 데이터와 재학습 파이프라인이 필요합니다.

즉, LLM은 사용자의 데이터를 자동으로 학습해서 발전하지 않습니다.

#### 2. "LLM이 검색도 하고 웹도 뒤져서 알려준다?"

요즘 Chat GPT 같은 AI 애플리케이션들을 사용하다보면 사용자의 질문에 웹 검색도 하고 웹 페이지도 뒤져서 답변을 해주는 것처럼 보입니다. 그리고 출처까지 제공합니다.
하지만 사실 웹을 검색하고 웹 페이지를 뒤지는 작업은 LLM이 하는 것이 아닙니다. LLM은 기본적으로 자연어를 자연스럽게 생성하는 것이지 이 이상의 일을 하지는 않습니다.

이렇게 웹을 뒤지거나 Github의 코드를 가져오거나 하는 등의 작업은 LLM이 아닌 뒤에서 설명할 Rag 기술이 하는 일입니다.


## Fine Tuning

LLM은 학습하지 않은 데이터에 대해서는 똑똑한 답변을 해줄 수 없습니다. 그런데 우리는 특정 업무 도메인에 맞는 AI를 만들고 싶을 때까 있습니다. 예를 들어 우리 회사의 업무에 특화된 AI 라던지, 병원 업무에 특화된 AI라던지, 하지만 그런데 LLM은 굉장히 범용적인 사용 용도이기 때문에 이런 특화 업무에는 약할 수 밖에 없습니다.
이럴때 우리는 업무 도메인에 특화된 데이터를 LLM에 추가적으로 학습을 시켜 우리 업무에 맞는 LLM을 만드는 것을 Fine Tuning이라고 합니다.


### Fine Tuning에 대한 오해

#### 1. "질문-답변 데이터 몇 개만 있으면 파인 튜닝 가능하다?"
파인 튜닝은 단순히 질문-답변 몇 줄을 넣는 작업이 아닙니다. 실제로는 수천~수만 개 이상의 고품질 데이터셋이 필요합니다.
JSON, CSV 등의 형식으로 정제된 구조화된 데이터가 요구됩니다. 데이터 내에 편향이나 오류가 있을 경우, 오히려 성능이 나빠질 수 있습니다.
즉, 데이터 준비가 파인 튜닝의 절반 이상을 차지합니다. 파인튜닝을 결코 쉽지 않습니다. LLM 모델을 처음부터 생성하는 것에 비해서는 쉽지만
파인 튜닝을 시키는 것 조자 엄청난 리소스가 필요합니다.

#### 2. "파인 튜닝하면 최신 정보가 반영된다?"

우리 회사의 업무에 대해서 파인 튜닝을 시켜놨으니 이제 우리 회사의 업무에 대해서 완벽하게 잘 답변해주겠지라고 생각할 수 있습니다. 하지만 파인 튜닝을 한 LLM도 결국 학습하지 않은 정보에 대해서는 똑똑한 답변을 해줄 수 없습니다.
예를 들어 우리 회사에서 A라는 업무를 하해서 A 업무에 대해 파인튜닝을 열심히 시켜 회사 특화된 LLM을 만들었는데 회사가 발전하면서 B에 대한 업무가 추가되었다고 가정해봅시다. 이러면 우리는 다시 B 업무에 대해서 데이터를 수천 ~ 수만개의 고품질 데이터를 준비해 이걸 또 다시 LLM에게 학습을 시켜야 합니다. 그래서 결국 파인 튜닝을 시킨다고 해서 우리 회사의 업무를 엄청 잘 할 수 있지 않습니다. 특히나 회사 정책이 자주 바뀐다면 파인 튜닝한 LLM은 무용지물이 되기 십상입니다.

## RAG(Retrieval-Augmented Generation)

RAG는 LLM의 한계를 극복하기 위해 개발된 기술입니다. RAG는 질문에 대한 답변을 제공할 때, 실시간으로 문서를 검색하여 LLM이 그 문서를 기반으로 정확한 답변을 생성하도록 돕습니다. 예를 들어, "올해 노벨 물리학상 수상자는 누구야?"라는 질문에 RAG는 최신 문서를 검색하여 "올해 노벨 물리학상은 피터 힉스에게 수여되었습니다."라는 답변을 제공합니다.

### RAG에 대한 오해

#### 1. "RAG 쓰면 답변이 항상 정확해진다?"

RAG는 정보를 검색해 LLM에 제공하는 구조지만, 검색이 정확하다고 해서 항상 정답을 말하는 건 아닙니다. 결국 외부에 검색을 했는데 검색 결과가 없거나 내용이 부정확하면 결국 잘못된 답변을 할 수 있습니다. 

#### 2. "RAG는 문서만 연결하면 바로 된다?"

